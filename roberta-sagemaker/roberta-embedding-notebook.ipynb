{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d010dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==3.0.2 essential_generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e467307",
   "metadata": {},
   "source": [
    "# Test sentence embedding with local deployment of \"Roberta-base\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c0ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 713 ms, sys: 35.9 ms, total: 749 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "# Roberta Model and Tokenizer\n",
    "from transformers import RobertaModel, RobertaTokenizer, AutoTokenizer, BertModel\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1020438b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.4 s, sys: 667 ms, total: 5.06 s\n",
      "Wall time: 8.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize tokenizer and model from HuggingFaces\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "PRE_TRAINED_MODEL_NAME = 'roberta-base'\n",
    "# CLASS_NAMES = ['negative', 'neutral', 'positive']\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "MAX_LEN = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28b843d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS is excited to announce that TorchServe is natively supported in Amazon SageMaker as the default model server for PyTorch inference\n",
      "CPU times: user 76 µs, sys: 0 ns, total: 76 µs\n",
      "Wall time: 80.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_data = {\"text\": \"AWS is excited to announce that TorchServe is natively supported in Amazon SageMaker as the default model server for PyTorch inference\"}\n",
    "review_text = test_data['text']\n",
    "print(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ca9c771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 ms, sys: 0 ns, total: 2.65 ms\n",
      "Wall time: 3.88 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "encoded_review = tokenizer.encode_plus(\n",
    "                                        review_text,\n",
    "                                        max_length=MAX_LEN,\n",
    "                                        add_special_tokens=True,\n",
    "                                        return_token_type_ids=False,\n",
    "                                        padding=True,\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt',\n",
    "                                        )\n",
    "\n",
    "input_ids = encoded_review['input_ids']\n",
    "attention_mask = encoded_review['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2ce77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 156 ms, sys: 7.86 ms, total: 164 ms\n",
      "Wall time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output = model(input_ids, attention_mask)\n",
    "\n",
    "embedding = pickle.dumps(output)\n",
    "decoded_embedding = pickle.loads(embedding)\n",
    "\n",
    "#print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74056084",
   "metadata": {},
   "source": [
    "## Save HuggingFaces model and tokenizer assets into local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda523b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d \"local_model_directory\" ]; then\n",
    "    rm -f local_model_directory/*    \n",
    "else\n",
    "    mkdir local_model_directory\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd9fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "local_model_path_var = os.path.join('./', 'local_model_directory')\n",
    "\n",
    "tokenizer.save_pretrained(local_model_path_var)\n",
    "model.save_pretrained(local_model_path_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b79cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 477M\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K Jul  6 18:25 .\n",
      "drwxrwxr-x 5 ec2-user ec2-user 4.0K Jul  6 18:25 ..\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  510 Jul  6 18:25 config.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 446K Jul  6 18:25 merges.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 476M Jul  6 18:25 pytorch_model.bin\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  772 Jul  6 18:25 special_tokens_map.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   25 Jul  6 18:25 tokenizer_config.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 878K Jul  6 18:25 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -alh {local_model_path_var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae4930fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"eks-sagemaker-us-east-1-bucket-mars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92715a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n",
      "merges.txt\n",
      "pytorch_model.bin\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "vocab.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "(cd local_model_directory ; tar -czvf roberta-base.tar.gz *)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff236124",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "mv local_model_directory/roberta-base.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe2a8564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./roberta-base.tar.gz to s3://eks-sagemaker-us-east-1-bucket-mars/roberta/roberta-base.tar.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 cp roberta-base.tar.gz s3://eks-sagemaker-us-east-1-bucket-mars/roberta/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307585ef",
   "metadata": {},
   "source": [
    "# Create 3000 copies of roberta-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24faf7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "#Replace S3 Bucket\n",
    "\n",
    "s3_bucket='eks-sagemaker-us-east-1-bucket-mars'\n",
    "\n",
    "for i in {0..2999}\n",
    "do\n",
    "  aws s3 cp roberta-base.tar.gz s3://$s3_bucket/roberta/roberta-base-$i.tar.gz \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccde3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all 3000 models loaded in bucket\n",
    "#!aws s3 ls s3://eks-sagemaker-us-east-1-bucket-mars/roberta/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03984a0",
   "metadata": {},
   "source": [
    "# SageMaker endpoint deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2263687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "sha256:8aa678f5d3963a9930ead4b72e29cff8017da98a0034d96a9c609bb961bbfd3c\n",
      "The push refers to repository [474422712127.dkr.ecr.us-east-1.amazonaws.com/nlp-sagemaker-multimodel]\n",
      "af83b501136f: Preparing\n",
      "07f2b2f89807: Preparing\n",
      "1560f87c4e4a: Preparing\n",
      "71690dc33eea: Preparing\n",
      "604dc4e32a22: Preparing\n",
      "81c0c646deb8: Preparing\n",
      "fec20c668cb5: Preparing\n",
      "03d61bce70c7: Preparing\n",
      "f6aa281a2333: Preparing\n",
      "8f8f0266f834: Preparing\n",
      "81c0c646deb8: Waiting\n",
      "fec20c668cb5: Waiting\n",
      "03d61bce70c7: Waiting\n",
      "f6aa281a2333: Waiting\n",
      "8f8f0266f834: Waiting\n",
      "07f2b2f89807: Pushed\n",
      "af83b501136f: Pushed\n",
      "1560f87c4e4a: Pushed\n",
      "71690dc33eea: Pushed\n",
      "604dc4e32a22: Pushed\n",
      "fec20c668cb5: Pushed\n",
      "03d61bce70c7: Pushed\n",
      "8f8f0266f834: Pushed\n",
      "f6aa281a2333: Pushed\n",
      "81c0c646deb8: Pushed\n",
      "latest: digest: sha256:8f63f21da4d19ba50d6f21fc05a7a38d19ed30a1a11ab65883894c3f1e1b87d3 size: 2407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=nlp-sagemaker-multimodel\n",
    "\n",
    "cd container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -q -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8451fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "s3_bucket = 'eks-sagemaker-us-east-1-bucket-mars'\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3d0ba",
   "metadata": {},
   "source": [
    "# Test out different instances here to create various endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4acc5",
   "metadata": {},
   "source": [
    "## Instance Recommendations for Multi-Model Endpoint (MME)\n",
    "\n",
    "Link: https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html#multi-model-endpoint-instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3443fca4",
   "metadata": {},
   "source": [
    "## Instance: ml.c5d.18xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9b1562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: roberta-multimodel-2021-07-13-02-58-56\n",
      "Model data Url: s3://eks-sagemaker-us-east-1-bucket-mars/roberta/\n",
      "Container image: 474422712127.dkr.ecr.us-east-1.amazonaws.com/nlp-sagemaker-multimodel:latest\n",
      "Model Arn: arn:aws:sagemaker:us-east-1:474422712127:model/roberta-multimodel-2021-07-13-02-58-56\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = 'roberta-multimodel-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = 's3://{}/roberta/'.format(s3_bucket) ## MODEL S3 URL\n",
    "container = '{}.dkr.ecr.{}.amazonaws.com/nlp-sagemaker-multimodel:latest'.format(account_id, region)\n",
    "instance_type = 'ml.c5d.18xlarge'\n",
    "\n",
    "print('Model name: ' + model_name)\n",
    "print('Model data Url: ' + model_url)\n",
    "print('Container image: ' + container)\n",
    "\n",
    "container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_url,\n",
    "    'Mode': 'MultiModel'\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container])\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5812fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: roberta-multimodel-config2021-07-13-02-59-00\n",
      "Endpoint config Arn: arn:aws:sagemaker:us-east-1:474422712127:endpoint-config/roberta-multimodel-config2021-07-13-02-59-00\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = 'roberta-multimodel-config' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': instance_type,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ced4609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: roberta-multimodel-endpoint2021-07-13-02-59-06\n",
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:474422712127:endpoint/roberta-multimodel-endpoint2021-07-13-02-59-06\n",
      "Endpoint Status: Creating\n",
      "Waiting for roberta-multimodel-endpoint2021-07-13-02-59-06 endpoint to be in service...\n",
      "CPU times: user 202 ms, sys: 10.3 ms, total: 212 ms\n",
      "Wall time: 7min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "endpoint_name = 'roberta-multimodel-endpoint' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddd86a",
   "metadata": {},
   "source": [
    "## Instance: ml.m5d.4xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ddd7d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: roberta-multimodel-2021-07-13-03-07-08\n",
      "Model data Url: s3://eks-sagemaker-us-east-1-bucket-mars/roberta/\n",
      "Container image: 474422712127.dkr.ecr.us-east-1.amazonaws.com/nlp-sagemaker-multimodel:latest\n",
      "Model Arn: arn:aws:sagemaker:us-east-1:474422712127:model/roberta-multimodel-2021-07-13-03-07-08\n"
     ]
    }
   ],
   "source": [
    "#Model Creation\n",
    "\n",
    "model_name = 'roberta-multimodel-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = 's3://{}/roberta/'.format(s3_bucket) ## MODEL S3 URL\n",
    "container = '{}.dkr.ecr.{}.amazonaws.com/nlp-sagemaker-multimodel:latest'.format(account_id, region)\n",
    "\n",
    "#change instance type\n",
    "instance_type = 'ml.m5d.4xlarge'\n",
    "\n",
    "print('Model name: ' + model_name)\n",
    "print('Model data Url: ' + model_url)\n",
    "print('Container image: ' + container)\n",
    "\n",
    "container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_url,\n",
    "    'Mode': 'MultiModel'\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container])\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae3cf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: roberta-multimodel-config2021-07-13-03-07-13\n",
      "Endpoint config Arn: arn:aws:sagemaker:us-east-1:474422712127:endpoint-config/roberta-multimodel-config2021-07-13-03-07-13\n"
     ]
    }
   ],
   "source": [
    "#Endpoint Config Creation\n",
    "endpoint_config_name = 'roberta-multimodel-config' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': instance_type,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d87dd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: roberta-multimodel-endpoint-INSTANCETWO2021-07-13-03-07-24\n",
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:474422712127:endpoint/roberta-multimodel-endpoint-instancetwo2021-07-13-03-07-24\n",
      "Endpoint Status: Creating\n",
      "Waiting for roberta-multimodel-endpoint-INSTANCETWO2021-07-13-03-07-24 endpoint to be in service...\n",
      "CPU times: user 184 ms, sys: 5.45 ms, total: 189 ms\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "endpoint_two_name = 'roberta-multimodel-endpoint-INSTANCETWO' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint name: ' + endpoint_two_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_two_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_two_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_two_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_two_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9b14084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Endpoints being tested\n",
    "endpoint_name = \"roberta-multimodel-endpoint2021-07-13-02-59-06\"\n",
    "endpoint_two_name = \"roberta-multimodel-endpoint-INSTANCETWO2021-07-13-03-07-24\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04344bda",
   "metadata": {},
   "source": [
    "## Creating Test Data for Endpoint Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc59e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from essential_generators import DocumentGenerator\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "gen = DocumentGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5d010",
   "metadata": {},
   "source": [
    "## Test Endpoint One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21a0b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "region = 'us-east-1'\n",
    "content_type = 'application/json'\n",
    "endpoint_name='roberta-multimodel-endpoint2021-07-13-02-59-06'\n",
    "target_model = \"roberta-base-0.tar.gz\"\n",
    "\n",
    "boto3config = Config(\n",
    "    retries={\n",
    "        'max_attempts': 100,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "payload = {\"text\": \"Matthias Tomczak 220,000. 89.2% of inhabitants\"}\n",
    "payload = json.dumps(payload)\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker-runtime',\n",
    "                                     config=boto3config,\n",
    "                                     region_name=region)\n",
    "\n",
    "response = sagemaker_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=payload,\n",
    "    ContentType=content_type,\n",
    "    TargetModel=target_model\n",
    ")\n",
    "response_body = response[\"Body\"].read()\n",
    "#print(response_body)\n",
    "print(\"working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad08579",
   "metadata": {},
   "source": [
    "## Test Endpoint Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8634ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "region = 'us-east-1'\n",
    "content_type = 'application/json'\n",
    "endpoint_two_name = \"roberta-multimodel-endpoint-INSTANCETWO2021-07-13-03-07-24\"\n",
    "target_model = \"roberta-base-0.tar.gz\"\n",
    "\n",
    "boto3config = Config(\n",
    "    retries={\n",
    "        'max_attempts': 100,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "payload = {\"text\": \"Matthias Tomczak 220,000. 89.2% of inhabitants\"}\n",
    "payload = json.dumps(payload)\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker-runtime',\n",
    "                                     config=boto3config,\n",
    "                                     region_name=region)\n",
    "\n",
    "response = sagemaker_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_two_name,\n",
    "    Body=payload,\n",
    "    ContentType=content_type,\n",
    "    TargetModel=target_model\n",
    ")\n",
    "response_body = response[\"Body\"].read()\n",
    "#print(response_body)\n",
    "print(\"working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61da086",
   "metadata": {},
   "source": [
    "# Load Testing Without Locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c90a8c1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 inferences for roberta-multimodel-endpoint2021-07-07-16-35-13 (max models: 10):\n",
      "Other 18th- international treaties, and ratifies diplomatic appointments. The federal government are subject to\n",
      "\n",
      "Errors - 0.0000 out of 100.0000 total runs | 0.0000% in 8.0442 seconds \n",
      "\n",
      "\n",
      "TPS: 12.4313\n",
      "Client end-to-end latency percentiles:\n",
      "Avg | P50 | P90 | P95 | P100\n",
      "77.9814 | 75.8916 | 82.5927 | 89.4579 | 158.1783 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from essential_generators import DocumentGenerator\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "gen = DocumentGenerator()\n",
    "gen.init_word_cache(5000)\n",
    "gen.init_sentence_cache(5000)\n",
    "\n",
    "total_runs = 100\n",
    "max_models = 10\n",
    "\n",
    "content_type = \"application/json\" \n",
    "accept = \"application/octet-stream\"\n",
    "\n",
    "print('Running {} inferences for {} (max models: {}):'.format(total_runs, endpoint_name, max_models))\n",
    "\n",
    "client_times = []\n",
    "errors_list = []\n",
    "\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "errors = 0\n",
    "\n",
    "print(gen.sentence())\n",
    "\n",
    "for _ in range(total_runs):\n",
    "    target_model = f'roberta-base-{random.randint(0,max_models)}.tar.gz'\n",
    "    test_data = {\"text\": gen.sentence()}\n",
    "    jsons = JSONSerializer()\n",
    "    payload = jsons.serialize(test_data)\n",
    "    client_start = time.time()\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=content_type,\n",
    "        TargetModel=target_model,\n",
    "        Body=payload)\n",
    "    client_end = time.time()\n",
    "    client_times.append((client_end - client_start)*1000)\n",
    "    skunk = response['Body'].read()\n",
    "    #print(len(skunk))  \n",
    "\n",
    "cw_end = datetime.datetime.utcnow()    \n",
    "\n",
    "cw_duration = cw_end - cw_start \n",
    "duration_in_s = cw_duration.total_seconds() \n",
    "\n",
    "tps = total_runs/duration_in_s\n",
    "\n",
    "print('\\nErrors - {:.4f} out of {:.4f} total runs | {:.4f}% in {:.4f} seconds \\n'.format(errors, total_runs, (errors/total_runs)*100, duration_in_s))\n",
    "errors = 0\n",
    "\n",
    "print('\\nTPS: {:.4f}'.format(tps))\n",
    "    \n",
    "print('Client end-to-end latency percentiles:')\n",
    "client_avg = np.mean(client_times)\n",
    "client_p50 = np.percentile(client_times, 50)\n",
    "client_p90 = np.percentile(client_times, 90)\n",
    "client_p95 = np.percentile(client_times, 95)\n",
    "client_p100 = np.percentile(client_times, 100)\n",
    "print('Avg | P50 | P90 | P95 | P100')\n",
    "print('{:.4f} | {:.4f} | {:.4f} | {:.4f} | {:.4f} \\n'.format(client_avg, client_p50, client_p90, client_p95, client_p100))\n",
    "\n",
    "# Give 5 minute buffer to end\n",
    "cw_end += datetime.timedelta(minutes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b106d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7128bddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: /home/ec2-user/anaconda3/envs/python3/bin/locust: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#cw_start = datetime.datetime.utcnow()\n",
    "#!/home/ec2-user/anaconda3/envs/python3/bin/locust -f locust_script.py -u 50 --headless --host=http://roberta-multimodel-endpoint2021-07-13-02-59-06 --stop-timeout 90 -L DEBUG -t 5m --logfile=logfile.log --csv=locustEndpointOne.csv --csv-full-history --reset-stats              \n",
    "#cw_end = datetime.datetime.utcnow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc81d7a",
   "metadata": {},
   "source": [
    "# Load Testing- Locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c95df61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f1408b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/bin/locust\n"
     ]
    }
   ],
   "source": [
    "!which locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5f34cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "region = 'us-east-1'\n",
    "content_type = 'application/json'\n",
    "endpoint_name='roberta-multimodel-endpoint2021-07-07-16-35-13'\n",
    "target_model = \"roberta-base-0.tar.gz\"\n",
    "\n",
    "boto3config = Config(\n",
    "    retries={\n",
    "        'max_attempts': 100,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "payload = {\"text\": \"Matthias Tomczak 220,000. 89.2% of inhabitants\"}\n",
    "payload = json.dumps(payload)\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker-runtime',\n",
    "                                     config=boto3config,\n",
    "                                     region_name=region)\n",
    "\n",
    "response = sagemaker_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=payload,\n",
    "    ContentType=content_type,\n",
    "    TargetModel=target_model\n",
    ")\n",
    "response_body = response[\"Body\"].read()\n",
    "print(\"working\")\n",
    "\n",
    "#print(response_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a0882",
   "metadata": {},
   "source": [
    "## Load Test Endpoint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab which locust location and endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e8f0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cw_start = datetime.datetime.utcnow()\n",
    "#!/home/ec2-user/anaconda3/envs/pytorch_p36/bin/locust -f locustfile.py -u 10 --headless --host=http://roberta-multimodel-endpoint2021-07-13-02-59-06 --stop-timeout 90 -L DEBUG -t 5m --logfile=logfile.log --csv=TestStats-Locust/locustEndpointOne.csv --csv-full-history --reset-stats              \n",
    "#cw_end = datetime.datetime.utcnow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3c69d",
   "metadata": {},
   "source": [
    "## Load Test Endpoint Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc916c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cw_start = datetime.datetime.utcnow()\n",
    "#!/home/ec2-user/anaconda3/envs/pytorch_p36/bin/locust -f locustfile.py -u 10 --headless --host=http://roberta-multimodel-endpoint-INSTANCETWO2021-07-13-03-07-24 --stop-timeout 90 -L DEBUG -t 5m --logfile=logfile.log --csv=locustEndpointTwo.csv --csv-full-history --reset-stats              \n",
    "#cw_end = datetime.datetime.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab0e6712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User Count</th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th>Requests/s</th>\n",
       "      <th>Failures/s</th>\n",
       "      <th>50%</th>\n",
       "      <th>66%</th>\n",
       "      <th>75%</th>\n",
       "      <th>80%</th>\n",
       "      <th>...</th>\n",
       "      <th>99.9%</th>\n",
       "      <th>99.99%</th>\n",
       "      <th>100%</th>\n",
       "      <th>Total Request Count</th>\n",
       "      <th>Total Failure Count</th>\n",
       "      <th>Total Median Response Time</th>\n",
       "      <th>Total Average Response Time</th>\n",
       "      <th>Total Min Response Time</th>\n",
       "      <th>Total Max Response Time</th>\n",
       "      <th>Total Average Content Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1626297691</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1626297692</td>\n",
       "      <td>1</td>\n",
       "      <td>custom_protocol_boto3</td>\n",
       "      <td>sagemaker_client_invoke_endpoint</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>102.250000</td>\n",
       "      <td>74</td>\n",
       "      <td>150</td>\n",
       "      <td>495500.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1626297692</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>102.250000</td>\n",
       "      <td>74</td>\n",
       "      <td>150</td>\n",
       "      <td>495500.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1626297693</td>\n",
       "      <td>2</td>\n",
       "      <td>custom_protocol_boto3</td>\n",
       "      <td>sagemaker_client_invoke_endpoint</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>120.045455</td>\n",
       "      <td>74</td>\n",
       "      <td>178</td>\n",
       "      <td>495501.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1626297693</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>120.045455</td>\n",
       "      <td>74</td>\n",
       "      <td>178</td>\n",
       "      <td>495501.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp  User Count                   Type  \\\n",
       "0  1626297691           0                    NaN   \n",
       "1  1626297692           1  custom_protocol_boto3   \n",
       "2  1626297692           1                    NaN   \n",
       "3  1626297693           2  custom_protocol_boto3   \n",
       "4  1626297693           2                    NaN   \n",
       "\n",
       "                               Name  Requests/s  Failures/s    50%    66%  \\\n",
       "0                        Aggregated         0.0         0.0    NaN    NaN   \n",
       "1  sagemaker_client_invoke_endpoint         0.0         0.0   97.0  100.0   \n",
       "2                        Aggregated         0.0         0.0   97.0  100.0   \n",
       "3  sagemaker_client_invoke_endpoint         0.0         0.0  120.0  130.0   \n",
       "4                        Aggregated         0.0         0.0  120.0  130.0   \n",
       "\n",
       "     75%    80%  ...  99.9%  99.99%   100%  Total Request Count  \\\n",
       "0    NaN    NaN  ...    NaN     NaN    NaN                    0   \n",
       "1  130.0  130.0  ...  150.0   150.0  150.0                    8   \n",
       "2  130.0  130.0  ...  150.0   150.0  150.0                    8   \n",
       "3  140.0  150.0  ...  180.0   180.0  180.0                   22   \n",
       "4  140.0  150.0  ...  180.0   180.0  180.0                   22   \n",
       "\n",
       "   Total Failure Count  Total Median Response Time  \\\n",
       "0                    0                           0   \n",
       "1                    0                          95   \n",
       "2                    0                          95   \n",
       "3                    0                         120   \n",
       "4                    0                         120   \n",
       "\n",
       "   Total Average Response Time  Total Min Response Time  \\\n",
       "0                     0.000000                        0   \n",
       "1                   102.250000                       74   \n",
       "2                   102.250000                       74   \n",
       "3                   120.045455                       74   \n",
       "4                   120.045455                       74   \n",
       "\n",
       "   Total Max Response Time  Total Average Content Size  \n",
       "0                        0                        0.00  \n",
       "1                      150                   495500.25  \n",
       "2                      150                   495500.25  \n",
       "3                      178                   495501.00  \n",
       "4                      178                   495501.00  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpointTwoLocust = pd.read_csv(\"locustEndpointTwo.csv_stats_history.csv\")\n",
    "endpointTwoLocust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9fe723a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th>Request Count</th>\n",
       "      <th>Failure Count</th>\n",
       "      <th>Median Response Time</th>\n",
       "      <th>Average Response Time</th>\n",
       "      <th>Min Response Time</th>\n",
       "      <th>Max Response Time</th>\n",
       "      <th>Average Content Size</th>\n",
       "      <th>Requests/s</th>\n",
       "      <th>...</th>\n",
       "      <th>66%</th>\n",
       "      <th>75%</th>\n",
       "      <th>80%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>98%</th>\n",
       "      <th>99%</th>\n",
       "      <th>99.9%</th>\n",
       "      <th>99.99%</th>\n",
       "      <th>100%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>custom_protocol_boto3</td>\n",
       "      <td>sagemaker_client_invoke_endpoint</td>\n",
       "      <td>8523</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>172.546404</td>\n",
       "      <td>121</td>\n",
       "      <td>3719</td>\n",
       "      <td>495501.205913</td>\n",
       "      <td>28.852287</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>200</td>\n",
       "      <td>210</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>1000</td>\n",
       "      <td>3700</td>\n",
       "      <td>3700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Type                              Name  Request Count  \\\n",
       "0  custom_protocol_boto3  sagemaker_client_invoke_endpoint           8523   \n",
       "\n",
       "   Failure Count  Median Response Time  Average Response Time  \\\n",
       "0              0                   170             172.546404   \n",
       "\n",
       "   Min Response Time  Max Response Time  Average Content Size  Requests/s  \\\n",
       "0                121               3719         495501.205913   28.852287   \n",
       "\n",
       "   ...  66%  75%  80%  90%  95%  98%  99%  99.9%  99.99%  100%  \n",
       "0  ...  180  190  190  200  210  220  220   1000    3700  3700  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locustE1Summary = pd.read_csv(\"locustEndpointOne.csv_stats.csv\")\n",
    "locustE1Summary = locustE1Summary.drop(1)\n",
    "locustE1Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1bac765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th>Request Count</th>\n",
       "      <th>Failure Count</th>\n",
       "      <th>Median Response Time</th>\n",
       "      <th>Average Response Time</th>\n",
       "      <th>Min Response Time</th>\n",
       "      <th>Max Response Time</th>\n",
       "      <th>Average Content Size</th>\n",
       "      <th>Requests/s</th>\n",
       "      <th>...</th>\n",
       "      <th>66%</th>\n",
       "      <th>75%</th>\n",
       "      <th>80%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>98%</th>\n",
       "      <th>99%</th>\n",
       "      <th>99.9%</th>\n",
       "      <th>99.99%</th>\n",
       "      <th>100%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>custom_protocol_boto3</td>\n",
       "      <td>sagemaker_client_invoke_endpoint</td>\n",
       "      <td>4926</td>\n",
       "      <td>0</td>\n",
       "      <td>580</td>\n",
       "      <td>591.231019</td>\n",
       "      <td>460</td>\n",
       "      <td>1328</td>\n",
       "      <td>495501.145757</td>\n",
       "      <td>16.910832</td>\n",
       "      <td>...</td>\n",
       "      <td>600</td>\n",
       "      <td>610</td>\n",
       "      <td>620</td>\n",
       "      <td>640</td>\n",
       "      <td>670</td>\n",
       "      <td>760</td>\n",
       "      <td>820</td>\n",
       "      <td>1200</td>\n",
       "      <td>1300</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Type                              Name  Request Count  \\\n",
       "0  custom_protocol_boto3  sagemaker_client_invoke_endpoint           4926   \n",
       "\n",
       "   Failure Count  Median Response Time  Average Response Time  \\\n",
       "0              0                   580             591.231019   \n",
       "\n",
       "   Min Response Time  Max Response Time  Average Content Size  Requests/s  \\\n",
       "0                460               1328         495501.145757   16.910832   \n",
       "\n",
       "   ...  66%  75%  80%  90%  95%  98%  99%  99.9%  99.99%  100%  \n",
       "0  ...  600  610  620  640  670  760  820   1200    1300  1300  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locustE2Summary = pd.read_csv(\"locustEndpointTwo.csv_stats.csv\")\n",
    "locustE2Summary = locustE2Summary.drop(1)\n",
    "locustE2Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0ae305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentilesValuesE1 = []\n",
    "percentilesValuesE1.append(list(locustE1Summary['50%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['66%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['75%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['80%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['90%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['95%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['98%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['99%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['99.9%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['99.99%'])[0])\n",
    "percentilesValuesE1.append(list(locustE1Summary['100%'])[0])\n",
    "\n",
    "\n",
    "percentilesValuesE2 = []\n",
    "percentilesValuesE2.append(list(locustE2Summary['50%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['66%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['75%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['80%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['90%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['95%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['98%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['99%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['99.9%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['99.99%'])[0])\n",
    "percentilesValuesE2.append(list(locustE2Summary['100%'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83385725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sns.set_style('darkgrid')\n",
    "X = ['50', '66', '75', '80', '90', '95', '98', '99', '99.9','99.99', '100']\n",
    "X_axis = np.arange(len(X))\n",
    "  \n",
    "plt.bar(X_axis - 0.2, percentilesValuesE1, 0.3, label = 'ml.c5d.18xlarge')\n",
    "plt.bar(X_axis + 0.2,percentilesValuesE2 , 0.3, label = 'ml.m5d.4xlarge')\n",
    "  \n",
    "plt.xticks(X_axis, X)\n",
    "plt.xlabel(\"Latency Percentile (%)\")\n",
    "plt.ylabel(\"E2E Latency\")\n",
    "plt.title(\"Comparing SM Compute Instances Optimize Latency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ead8bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th>Request Count</th>\n",
       "      <th>Failure Count</th>\n",
       "      <th>Median Response Time</th>\n",
       "      <th>Average Response Time</th>\n",
       "      <th>Min Response Time</th>\n",
       "      <th>Max Response Time</th>\n",
       "      <th>Average Content Size</th>\n",
       "      <th>Requests/s</th>\n",
       "      <th>...</th>\n",
       "      <th>66%</th>\n",
       "      <th>75%</th>\n",
       "      <th>80%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>98%</th>\n",
       "      <th>99%</th>\n",
       "      <th>99.9%</th>\n",
       "      <th>99.99%</th>\n",
       "      <th>100%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>custom_protocol_boto3</td>\n",
       "      <td>sagemaker_client_invoke_endpoint</td>\n",
       "      <td>8617</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>170.557851</td>\n",
       "      <td>109</td>\n",
       "      <td>1313</td>\n",
       "      <td>495501.227341</td>\n",
       "      <td>29.186077</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>240</td>\n",
       "      <td>1300</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>8617</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>170.557851</td>\n",
       "      <td>109</td>\n",
       "      <td>1313</td>\n",
       "      <td>495501.227341</td>\n",
       "      <td>29.186077</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>240</td>\n",
       "      <td>1300</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Type                              Name  Request Count  \\\n",
       "0  custom_protocol_boto3  sagemaker_client_invoke_endpoint           8617   \n",
       "1                    NaN                        Aggregated           8617   \n",
       "\n",
       "   Failure Count  Median Response Time  Average Response Time  \\\n",
       "0              0                   170             170.557851   \n",
       "1              0                   170             170.557851   \n",
       "\n",
       "   Min Response Time  Max Response Time  Average Content Size  Requests/s  \\\n",
       "0                109               1313         495501.227341   29.186077   \n",
       "1                109               1313         495501.227341   29.186077   \n",
       "\n",
       "   ...  66%  75%  80%  90%  95%  98%  99%  99.9%  99.99%  100%  \n",
       "0  ...  180  190  190  200  200  220  220    240    1300  1300  \n",
       "1  ...  180  190  190  200  200  220  220    240    1300  1300  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locustSummary = pd.read_csv('locust.csv_stats.csv')\n",
    "locustSummary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "418d9d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'Name', 'Request Count', 'Failure Count',\n",
       "       'Median Response Time', 'Average Response Time', 'Min Response Time',\n",
       "       'Max Response Time', 'Average Content Size', 'Requests/s', 'Failures/s',\n",
       "       '50%', '66%', '75%', '80%', '90%', '95%', '98%', '99%', '99.9%',\n",
       "       '99.99%', '100%'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locustSummary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1231fef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th>Request Count</th>\n",
       "      <th>Failure Count</th>\n",
       "      <th>Median Response Time</th>\n",
       "      <th>Average Response Time</th>\n",
       "      <th>Min Response Time</th>\n",
       "      <th>Max Response Time</th>\n",
       "      <th>Average Content Size</th>\n",
       "      <th>Requests/s</th>\n",
       "      <th>...</th>\n",
       "      <th>66%</th>\n",
       "      <th>75%</th>\n",
       "      <th>80%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>98%</th>\n",
       "      <th>99%</th>\n",
       "      <th>99.9%</th>\n",
       "      <th>99.99%</th>\n",
       "      <th>100%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>custom_protocol_boto3</td>\n",
       "      <td>sagemaker_client_invoke_endpoint</td>\n",
       "      <td>8617</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>170.557851</td>\n",
       "      <td>109</td>\n",
       "      <td>1313</td>\n",
       "      <td>495501.227341</td>\n",
       "      <td>29.186077</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>240</td>\n",
       "      <td>1300</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Type                              Name  Request Count  \\\n",
       "0  custom_protocol_boto3  sagemaker_client_invoke_endpoint           8617   \n",
       "\n",
       "   Failure Count  Median Response Time  Average Response Time  \\\n",
       "0              0                   170             170.557851   \n",
       "\n",
       "   Min Response Time  Max Response Time  Average Content Size  Requests/s  \\\n",
       "0                109               1313         495501.227341   29.186077   \n",
       "\n",
       "   ...  66%  75%  80%  90%  95%  98%  99%  99.9%  99.99%  100%  \n",
       "0  ...  180  190  190  200  200  220  220    240    1300  1300  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locustSummary = locustSummary.drop(1)\n",
    "locustSummary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e02bf3",
   "metadata": {},
   "source": [
    "# CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9134bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "statistics = ['Sum', 'SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended = ['p50', 'p90', 'p95', 'p99', 'p100']\n",
    "\n",
    "metrics_to_gather = []\n",
    "metrics_to_gather.append('CPUUtilization')\n",
    "metrics_to_gather.append('MemoryUtilization')\n",
    "metrics_to_gather.append('GPUUtilization')\n",
    "metrics_to_gather.append('GPUMemoryUtilization')\n",
    "metrics_to_gather.append('DiskUtilization')\n",
    "metrics_to_gather.append('ModelLatency')\n",
    "metrics_to_gather.append('OverheadLatency')\n",
    "metrics_to_gather.append('Invocations')\n",
    "metrics_to_gather.append('Invocation4XXErrors')\n",
    "metrics_to_gather.append('Invocation5XXErrors')\n",
    "metrics_to_gather.append('InvocationsPerInstance')\n",
    "\n",
    "def get_sample_count(cw_end, cw_start):\n",
    "    \n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "    metrics_to_gather=['Invocations']\n",
    "    statistics=['SampleCount']\n",
    "    \n",
    "    factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "    period = factor * 60\n",
    "    \n",
    "    cloudwatch_ready = False\n",
    "    \n",
    "    # Keep polling CloudWatch metrics until datapoints are available\n",
    "    while not cloudwatch_ready:\n",
    "        time.sleep(90)\n",
    "        for metric in metrics_to_gather:\n",
    "            model_latency_metrics = cloudwatch.get_metric_statistics(MetricName=metric,\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics\n",
    "                                             )\n",
    "            if len(model_latency_metrics['Datapoints']) > 0:\n",
    "                samplecount = model_latency_metrics['Datapoints'][0]['SampleCount']\n",
    "                cloudwatch_ready = True\n",
    "    \n",
    "    return(samplecount)\n",
    "\n",
    "def collect_cloudwatch_metrics(statistics, extended, metrics_to_gather, total_runs, cw_end, cw_start):\n",
    "    \n",
    "    print('Getting Cloudwatch:')\n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "    # Period must be 1, 5, 10, 30, or multiple of 60\n",
    "    # Calculate closest multiple of 60 to the total elapsed time\n",
    "    factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "    period = factor * 60\n",
    "    print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "    print('Using period of {} seconds\\n'.format(period))\n",
    "\n",
    "    cloudwatch_ready = False\n",
    "    \n",
    "    # Keep polling CloudWatch metrics until datapoints are available\n",
    "    while not cloudwatch_ready:\n",
    "        \n",
    "        time.sleep(90)\n",
    "        \n",
    "        print('Waiting 30 seconds ...')\n",
    "\n",
    "        for metric in metrics_to_gather:\n",
    "            \n",
    "            if(metric.find('Util') != -1):\n",
    "                namespace = \"/aws/sagemaker/Endpoints\"\n",
    "            else:\n",
    "                namespace = \"AWS/SageMaker\"\n",
    "            \n",
    "            model_latency_metrics = cloudwatch.get_metric_statistics(MetricName=metric,\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=namespace,\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )            \n",
    "            #print(metric)\n",
    "            if len(model_latency_metrics['Datapoints']) > 0:\n",
    "                #print(model_latency_metrics)\n",
    "                print(metric +'\\n')\n",
    "                side_avg = model_latency_metrics['Datapoints'][0]['Average'] / total_runs\n",
    "                side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / total_runs\n",
    "                side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / total_runs\n",
    "                side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / total_runs\n",
    "                side_p99 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p99'] / total_runs\n",
    "                side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / total_runs\n",
    "\n",
    "                sumcount = model_latency_metrics['Datapoints'][0]['Sum']\n",
    "                samplecount = model_latency_metrics['Datapoints'][0]['SampleCount']\n",
    "                average = model_latency_metrics['Datapoints'][0]['Average']\n",
    "                minimum = model_latency_metrics['Datapoints'][0]['Minimum']\n",
    "                maximum = model_latency_metrics['Datapoints'][0]['Maximum']\n",
    "\n",
    "                #statistics = ['Sum', 'SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "                #extended = ['p50', 'p90', 'p95', 'p99', 'p100']\n",
    "                print('Avg | P50 | P90 | P95 | P95 | P100')\n",
    "                print('{:.4f} | {:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(side_avg, side_p50, side_p90, side_p95, side_p99, side_p100))\n",
    "                print('Sum | SampleCount | Average | Minimum | Maximum')\n",
    "                print('{:.4f} | {:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(sumcount, samplecount, average, minimum, maximum))\n",
    "                \n",
    "                cloudwatch_ready = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25fc7a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Cloudwatch:\n",
      "Time elapsed: 302.010715 seconds\n",
      "Using period of 360 seconds\n",
      "\n",
      "Waiting 30 seconds ...\n",
      "CPUUtilization\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.0580 | 0.0582 | 0.0585 | 0.0586 | 0.0586\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "1.4506 | 5.0000 | 0.2901 | 0.2870 | 0.2932\n",
      "\n",
      "MemoryUtilization\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.3236 | 0.3236 | 0.3237 | 0.3237 | 0.3237\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "8.0904 | 5.0000 | 1.6181 | 1.6178 | 1.6184\n",
      "\n",
      "DiskUtilization\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.0071 | 0.0071 | 0.0071 | 0.0071 | 0.0071\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "0.1766 | 5.0000 | 0.0353 | 0.0353 | 0.0353\n",
      "\n",
      "Invocations\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "0.0000 | 5.0000 | 0.0000 | 0.0000 | 0.0000\n",
      "\n",
      "InvocationsPerInstance\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "0.0000 | 5.0000 | 0.0000 | 0.0000 | 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_runs = get_sample_count(cw_end, cw_start)\n",
    "collect_cloudwatch_metrics(statistics, extended, metrics_to_gather, total_runs, cw_end, cw_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6486c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "statistics = ['Sum', 'SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended = ['p50', 'p90', 'p95', 'p99', 'p100']\n",
    "\n",
    "metrics_to_gather = []\n",
    "metrics_to_gather.append('CPUUtilization')\n",
    "metrics_to_gather.append('MemoryUtilization')\n",
    "metrics_to_gather.append('GPUUtilization')\n",
    "metrics_to_gather.append('GPUMemoryUtilization')\n",
    "metrics_to_gather.append('DiskUtilization')\n",
    "metrics_to_gather.append('ModelLatency')\n",
    "metrics_to_gather.append('OverheadLatency')\n",
    "metrics_to_gather.append('Invocations')\n",
    "metrics_to_gather.append('Invocation4XXErrors')\n",
    "metrics_to_gather.append('Invocation5XXErrors')\n",
    "metrics_to_gather.append('InvocationsPerInstance')\n",
    "\n",
    "def get_sample_count(cw_end, cw_start):\n",
    "    \n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "    metrics_to_gather=['Invocations']\n",
    "    statistics=['SampleCount']\n",
    "    \n",
    "    factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "    period = factor * 60\n",
    "    \n",
    "    cloudwatch_ready = False\n",
    "    \n",
    "    # Keep polling CloudWatch metrics until datapoints are available\n",
    "    while not cloudwatch_ready:\n",
    "        time.sleep(90)\n",
    "        for metric in metrics_to_gather:\n",
    "            model_latency_metrics = cloudwatch.get_metric_statistics(MetricName=metric,\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics\n",
    "                                             )\n",
    "            if len(model_latency_metrics['Datapoints']) > 0:\n",
    "                samplecount = model_latency_metrics['Datapoints'][0]['SampleCount']\n",
    "                cloudwatch_ready = True\n",
    "    \n",
    "    return(samplecount)\n",
    "\n",
    "def collect_cloudwatch_metrics(statistics, extended, metrics_to_gather, total_runs, cw_end, cw_start):\n",
    "    \n",
    "    print('Getting Cloudwatch:')\n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "    # Period must be 1, 5, 10, 30, or multiple of 60\n",
    "    # Calculate closest multiple of 60 to the total elapsed time\n",
    "    factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "    period = factor * 60\n",
    "    print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "    print('Using period of {} seconds\\n'.format(period))\n",
    "\n",
    "    cloudwatch_ready = False\n",
    "    \n",
    "    # Keep polling CloudWatch metrics until datapoints are available\n",
    "    while not cloudwatch_ready:\n",
    "        \n",
    "        time.sleep(90)\n",
    "        \n",
    "        print('Waiting 30 seconds ...')\n",
    "\n",
    "        for metric in metrics_to_gather:\n",
    "            \n",
    "            if(metric.find('Util') != -1):\n",
    "                namespace = \"/aws/sagemaker/Endpoints\"\n",
    "            else:\n",
    "                namespace = \"AWS/SageMaker\"\n",
    "            \n",
    "            model_latency_metrics = cloudwatch.get_metric_statistics(MetricName=metric,\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': endpoint_two_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=namespace,\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )            \n",
    "            #print(metric)\n",
    "            if len(model_latency_metrics['Datapoints']) > 0:\n",
    "                #print(model_latency_metrics)\n",
    "                print(metric +'\\n')\n",
    "                side_avg = model_latency_metrics['Datapoints'][0]['Average'] / total_runs\n",
    "                side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / total_runs\n",
    "                side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / total_runs\n",
    "                side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / total_runs\n",
    "                side_p99 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p99'] / total_runs\n",
    "                side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / total_runs\n",
    "\n",
    "                sumcount = model_latency_metrics['Datapoints'][0]['Sum']\n",
    "                samplecount = model_latency_metrics['Datapoints'][0]['SampleCount']\n",
    "                average = model_latency_metrics['Datapoints'][0]['Average']\n",
    "                minimum = model_latency_metrics['Datapoints'][0]['Minimum']\n",
    "                maximum = model_latency_metrics['Datapoints'][0]['Maximum']\n",
    "\n",
    "                #statistics = ['Sum', 'SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "                #extended = ['p50', 'p90', 'p95', 'p99', 'p100']\n",
    "                print('Avg | P50 | P90 | P95 | P95 | P100')\n",
    "                print('{:.4f} | {:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(side_avg, side_p50, side_p90, side_p95, side_p99, side_p100))\n",
    "                print('Sum | SampleCount | Average | Minimum | Maximum')\n",
    "                print('{:.4f} | {:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(sumcount, samplecount, average, minimum, maximum))\n",
    "                \n",
    "                cloudwatch_ready = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15d15b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Cloudwatch:\n",
      "Time elapsed: 302.010715 seconds\n",
      "Using period of 360 seconds\n",
      "\n",
      "Waiting 30 seconds ...\n",
      "CPUUtilization\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "217.1179 | 257.6641 | 260.4823 | 260.8368 | 261.1206\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "7816.2460 | 6.0000 | 1302.7077 | 496.0960 | 1567.1500\n",
      "\n",
      "MemoryUtilization\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.5438 | 0.5432 | 0.5593 | 0.5613 | 0.5630\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "19.5784 | 6.0000 | 3.2631 | 3.1427 | 3.3804\n",
      "\n",
      "DiskUtilization\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.0173 | 0.0173 | 0.0174 | 0.0174 | 0.0174\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "0.6246 | 6.0000 | 0.1041 | 0.1036 | 0.1046\n",
      "\n",
      "ModelLatency\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "94805.6104 | 93964.4762 | 105356.8075 | 111512.2272 | 132965.9511\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "2873747663.0000 | 5052.0000 | 568833.6625 | 63195.0000 | 1304678.0000\n",
      "\n",
      "OverheadLatency\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "900.2121 | 720.6551 | 991.6399 | 1224.9492 | 3483.3996\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "27287228.0000 | 5052.0000 | 5401.2724 | 2789.0000 | 606288.0000\n",
      "\n",
      "Invocations\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.1667 | 0.1667 | 0.1667 | 0.1667 | 0.1667\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "5052.0000 | 5052.0000 | 1.0000 | 1.0000 | 1.0000\n",
      "\n",
      "Invocation4XXErrors\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "0.0000 | 5052.0000 | 0.0000 | 0.0000 | 0.0000\n",
      "\n",
      "Invocation5XXErrors\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "0.0000 | 5052.0000 | 0.0000 | 0.0000 | 0.0000\n",
      "\n",
      "InvocationsPerInstance\n",
      "\n",
      "Avg | P50 | P90 | P95 | P95 | P100\n",
      "0.1667 | 0.1667 | 0.1667 | 0.1667 | 0.1667\n",
      "\n",
      "Sum | SampleCount | Average | Minimum | Maximum\n",
      "5052.0000 | 5052.0000 | 1.0000 | 1.0000 | 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_runs = get_sample_count(cw_end, cw_start)\n",
    "collect_cloudwatch_metrics(statistics, extended, metrics_to_gather, total_runs, cw_end, cw_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
